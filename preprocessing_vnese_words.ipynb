{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_n-grams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Pp8A6d5F0Uo266YRxVeT1h4O0vQUIAf9",
      "authorship_tag": "ABX9TyM8UCDzLC8617E++WK/RxXM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hungtran-pro/Sentiment_Classification_VNese/blob/main/preprocessing_vnese_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using N-gram vs Bayes."
      ],
      "metadata": {
        "id": "QbVnuOQLZsYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bSSH3N7a_8Z",
        "outputId": "c36f3e13-efc4-49f6-ae4f-ad12a288d9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.5 MB 15.0 MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.63.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 965 kB 3.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import f1_score\n",
        "import string\n",
        "from pyvi import ViTokenizer\n",
        "import codecs\n",
        "import os"
      ],
      "metadata": {
        "id": "3lGoUf5aabiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqUOpEev6Ypo"
      },
      "outputs": [],
      "source": [
        "def __getData__(path):\n",
        "    x_train = []\n",
        "    y_train = []\n",
        "    f = open(path, \"r\")\n",
        "    raw_data = f.read()\n",
        "\n",
        "    lst_raw_data = raw_data.split(\"\\n\")\n",
        "    lst_raw_data.pop()\n",
        "    # print(lst_raw_data[0].split(\" \")[0].split(\"__\")[2])\n",
        "    for sentence in lst_raw_data:\n",
        "        lst_words = sentence.split(\" \")\n",
        "        # print(\" \".join(lst_words[1:]))\n",
        "        x_train.append(\" \".join(lst_words[1:]))\n",
        "        y_train.append(lst_words[0].split(\"__\")[2])\n",
        "  \n",
        "    f.close()      \n",
        "    return x_train, y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VN_CHARS_LOWER = u'·∫°·∫£√£√†√°√¢·∫≠·∫ß·∫•·∫©·∫´ƒÉ·∫Ø·∫±·∫∑·∫≥·∫µ√≥√≤·ªç√µ·ªè√¥·ªô·ªï·ªó·ªì·ªë∆°·ªù·ªõ·ª£·ªü·ª°√©√®·∫ª·∫π·∫Ω√™·∫ø·ªÅ·ªá·ªÉ·ªÖ√∫√π·ª•·ªß≈©∆∞·ª±·ªØ·ª≠·ª´·ª©√≠√¨·ªã·ªâƒ©√Ω·ª≥·ª∑·ªµ·ªπƒë√∞'\n",
        "VN_CHARS_UPPER = u'·∫†·∫¢√É√Ä√Å√Ç·∫¨·∫¶·∫§·∫®·∫™ƒÇ·∫Æ·∫∞·∫∂·∫≤·∫¥√ì√í·ªå√ï·ªé√î·ªò·ªî·ªñ·ªí·ªê∆†·ªú·ªö·ª¢·ªû·ª†√â√à·∫∫·∫∏·∫º√ä·∫æ·ªÄ·ªÜ·ªÇ·ªÑ√ö√ô·ª§·ª¶≈®∆Ø·ª∞·ªÆ·ª¨·ª™·ª®√ç√å·ªä·ªàƒ®√ù·ª≤·ª∂·ª¥·ª∏√êƒê'\n",
        "VN_CHARS = VN_CHARS_LOWER + VN_CHARS_UPPER\n",
        "\n",
        "path_nag = '/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/nag.txt'\n",
        "path_pos = '/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/pos.txt'\n",
        "path_not = '/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/not.txt'\n",
        "\n",
        "with codecs.open(path_nag, 'r', encoding='UTF-8') as f:\n",
        "    nag = f.readlines()\n",
        "nag_list = [n.replace('\\n', '') for n in nag]\n",
        "\n",
        "with codecs.open(path_pos, 'r', encoding='UTF-8') as f:\n",
        "    pos = f.readlines()\n",
        "pos_list = [n.replace('\\n', '') for n in pos]\n",
        "with codecs.open(path_not, 'r', encoding='UTF-8') as f:\n",
        "    not_ = f.readlines()\n",
        "not_list = [n.replace('\\n', '') for n in not_]"
      ],
      "metadata": {
        "id": "QC_uju5rZ4rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def no_marks(s):\n",
        "    __INTAB = [ch for ch in VN_CHARS]\n",
        "    __OUTTAB = \"a\"*17 + \"o\"*17 + \"e\"*11 + \"u\"*11 + \"i\"*5 + \"y\"*5 + \"d\"*2\n",
        "    __OUTTAB += \"A\"*17 + \"O\"*17 + \"E\"*11 + \"U\"*11 + \"I\"*5 + \"Y\"*5 + \"D\"*2\n",
        "    __r = re.compile(\"|\".join(__INTAB))\n",
        "    __replaces_dict = dict(zip(__INTAB, __OUTTAB))\n",
        "    result = __r.sub(lambda m: __replaces_dict[m.group(0)], s)\n",
        "    return result"
      ],
      "metadata": {
        "id": "SsnmVsQWaZHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "\n",
        "    #Remove link\n",
        "    text = ' '.join([w for w in text.split(\" \") if len(w) < 8])\n",
        "    #Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp\n",
        "    text = re.sub(r'([A-Z])\\1+', lambda m: m.group(1).upper(), text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng\n",
        "    text = text.lower()\n",
        "\n",
        "    #Chu·∫©n h√≥a ti·∫øng Vi·ªát, x·ª≠ l√Ω emoj, chu·∫©n h√≥a ti·∫øng Anh, thu·∫≠t ng·ªØ\n",
        "    replace_list = {\n",
        "        '√≤a': 'o√†', '√≥a': 'o√°', '·ªèa': 'o·∫£', '√µa': 'o√£', '·ªça': 'o·∫°', '√≤e': 'o√®', '√≥e': 'o√©','·ªèe': 'o·∫ª',\n",
        "        '√µe': 'o·∫Ω', '·ªçe': 'o·∫π', '√πy': 'u·ª≥', '√∫y': 'u√Ω', '·ªßy': 'u·ª∑', '≈©y': 'u·ªπ','·ª•y': 'u·ªµ', 'u·∫£': '·ªßa',\n",
        "        'aÃâ': '·∫£', '√¥ÃÅ': '·ªë', 'u¬¥': '·ªë','√¥ÃÉ': '·ªó', '√¥ÃÄ': '·ªì', '√¥Ãâ': '·ªï', '√¢ÃÅ': '·∫•', '√¢ÃÉ': '·∫´', '√¢Ãâ': '·∫©',\n",
        "        '√¢ÃÄ': '·∫ß', 'oÃâ': '·ªè', '√™ÃÄ': '·ªÅ','√™ÃÉ': '·ªÖ', 'ƒÉÃÅ': '·∫Ø', 'uÃâ': '·ªß', '√™ÃÅ': '·∫ø', '∆°Ãâ': '·ªü', 'iÃâ': '·ªâ',\n",
        "        'eÃâ': '·∫ª', '√†k': u' √† ','aÀã': '√†', 'iÀã': '√¨', 'ƒÉ¬¥': '·∫Ø','∆∞Ãâ': '·ª≠', 'eÀú': '·∫Ω', 'yÀú': '·ªπ', 'a¬¥': '√°',\n",
        "        #Quy c√°c icon v·ªÅ 2 lo·∫°i emoj: T√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c\n",
        "        \"üëπ\": \"nagative\", \"üëª\": \"positive\", \"üíÉ\": \"positive\",'ü§ô': ' positive ', 'üëç': ' positive ',\n",
        "        \"üíÑ\": \"positive\", \"üíé\": \"positive\", \"üí©\": \"positive\",\"üòï\": \"nagative\", \"üò±\": \"nagative\", \"üò∏\": \"positive\",\n",
        "        \"üòæ\": \"nagative\", \"üö´\": \"nagative\",  \"ü§¨\": \"nagative\",\"üßö\": \"positive\", \"üß°\": \"positive\",'üê∂':' positive ',\n",
        "        'üëé': ' nagative ', 'üò£': ' nagative ','‚ú®': ' positive ', '‚ù£': ' positive ','‚òÄ': ' positive ',\n",
        "        '‚ô•': ' positive ', 'ü§©': ' positive ', 'like': ' positive ', 'üíå': ' positive ',\n",
        "        'ü§£': ' positive ', 'üñ§': ' positive ', 'ü§§': ' positive ', ':(': ' nagative ', 'üò¢': ' nagative ',\n",
        "        '‚ù§': ' positive ', 'üòç': ' positive ', 'üòò': ' positive ', 'üò™': ' nagative ', 'üòä': ' positive ',\n",
        "        '?': ' ? ', 'üòÅ': ' positive ', 'üíñ': ' positive ', 'üòü': ' nagative ', 'üò≠': ' nagative ',\n",
        "        'üíØ': ' positive ', 'üíó': ' positive ', '‚ô°': ' positive ', 'üíú': ' positive ', 'ü§ó': ' positive ',\n",
        "        '^^': ' positive ', 'üò®': ' nagative ', '‚ò∫': ' positive ', 'üíã': ' positive ', 'üëå': ' positive ',\n",
        "        'üòñ': ' nagative ', 'üòÄ': ' positive ', ':((': ' nagative ', 'üò°': ' nagative ', 'üò†': ' nagative ',\n",
        "        'üòí': ' nagative ', 'üôÇ': ' positive ', 'üòè': ' nagative ', 'üòù': ' positive ', 'üòÑ': ' positive ',\n",
        "        'üòô': ' positive ', 'üò§': ' nagative ', 'üòé': ' positive ', 'üòÜ': ' positive ', 'üíö': ' positive ',\n",
        "        '‚úå': ' positive ', 'üíï': ' positive ', 'üòû': ' nagative ', 'üòì': ' nagative ', 'Ô∏èüÜóÔ∏è': ' positive ',\n",
        "        'üòâ': ' positive ', 'üòÇ': ' positive ', ':v': '  positive ', '=))': '  positive ', 'üòã': ' positive ',\n",
        "        'üíì': ' positive ', 'üòê': ' nagative ', ':3': ' positive ', 'üò´': ' nagative ', 'üò•': ' nagative ',\n",
        "        'üòÉ': ' positive ', 'üò¨': ' üò¨ ', 'üòå': ' üòå ', 'üíõ': ' positive ', 'ü§ù': ' positive ', 'üéà': ' positive ',\n",
        "        'üòó': ' positive ', 'ü§î': ' nagative ', 'üòë': ' nagative ', 'üî•': ' nagative ', 'üôè': ' nagative ',\n",
        "        'üÜó': ' positive ', 'üòª': ' positive ', 'üíô': ' positive ', 'üíü': ' positive ',\n",
        "        'üòö': ' positive ', '‚ùå': ' nagative ', 'üëè': ' positive ', ';)': ' positive ', '<3': ' positive ',\n",
        "        'üåù': ' positive ',  'üå∑': ' positive ', 'üå∏': ' positive ', 'üå∫': ' positive ',\n",
        "        'üåº': ' positive ', 'üçì': ' positive ', 'üêÖ': ' positive ', 'üêæ': ' positive ', 'üëâ': ' positive ',\n",
        "        'üíê': ' positive ', 'üíû': ' positive ', 'üí•': ' positive ', 'üí™': ' positive ',\n",
        "        'üí∞': ' positive ',  'üòá': ' positive ', 'üòõ': ' positive ', 'üòú': ' positive ',\n",
        "        'üôÉ': ' positive ', 'ü§ë': ' positive ', 'ü§™': ' positive ','‚òπ': ' nagative ',  'üíÄ': ' nagative ',\n",
        "        'üòî': ' nagative ', 'üòß': ' nagative ', 'üò©': ' nagative ', 'üò∞': ' nagative ', 'üò≥': ' nagative ',\n",
        "        'üòµ': ' nagative ', 'üò∂': ' nagative ', 'üôÅ': ' nagative ',\n",
        "        #Chu·∫©n h√≥a 1 s·ªë sentiment words/English words\n",
        "        ':))': '  positive ', ':)': ' positive ', '√¥ k√™i': ' ok ', 'okie': ' ok ', ' o k√™ ': ' ok ',\n",
        "        'okey': ' ok ', '√¥k√™': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','ok√™':' ok ',\n",
        "        ' tks ': u' c√°m ∆°n ', 'thks': u' c√°m ∆°n ', 'thanks': u' c√°m ∆°n ', 'ths': u' c√°m ∆°n ', 'thank': u' c√°m ∆°n ',\n",
        "        '‚≠ê': 'star ', '*': 'star ', 'üåü': 'star ', 'üéâ': u' positive ',\n",
        "        'kg ': u' kh√¥ng ','not': u' kh√¥ng ', u' kg ': u' kh√¥ng ', '\"k ': u' kh√¥ng ',' kh ':u' kh√¥ng ','k√¥':u' kh√¥ng ','hok':u' kh√¥ng ',' kp ': u' kh√¥ng ph·∫£i ',u' k√¥ ': u' kh√¥ng ', '\"ko ': u' kh√¥ng ', u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', 'khong': u' kh√¥ng ', u' hok ': u' kh√¥ng ',\n",
        "        'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',\n",
        "        ' lol ': ' nagative ',' cc ': ' nagative ','cute': u' d·ªÖ th∆∞∆°ng ','huhu': ' nagative ', ' vs ': u' v·ªõi ', 'wa': ' qu√° ', 'w√°': u' qu√°', 'j': u' g√¨ ', '‚Äú': ' ',\n",
        "        ' sz ': u' c·ª° ', 'size': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', 'dk': u' ƒë∆∞·ª£c ', 'dc': u' ƒë∆∞·ª£c ', 'ƒëk': u' ƒë∆∞·ª£c ',\n",
        "        'ƒëc': u' ƒë∆∞·ª£c ','authentic': u' chu·∫©n ch√≠nh h√£ng ',u' aut ': u' chu·∫©n ch√≠nh h√£ng ', u' auth ': u' chu·∫©n ch√≠nh h√£ng ', 'thick': u' positive ', 'store': u' c·ª≠a h√†ng ',\n",
        "        'shop': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ','god': u' t·ªët ','wel done':' t·ªët ', 'good': u' t·ªët ', 'g√∫t': u' t·ªët ',\n",
        "        's·∫•u': u' x·∫•u ','gut': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', 'perfect': 'r·∫•t t·ªët', 'bt': u' b√¨nh th∆∞·ªùng ',\n",
        "        'time': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',\n",
        "        '√™Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', 'quality': 'ch·∫•t l∆∞·ª£ng','chat':' ch·∫•t ', 'excelent': 'ho√†n h·∫£o', 'bad': 't·ªá','fresh': ' t∆∞∆°i ','sad': ' t·ªá ',\n",
        "        'date': u' h·∫°n s·ª≠ d·ª•ng ', 'hsd': u' h·∫°n s·ª≠ d·ª•ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao h√†ng ',u' s√≠p ': u' giao h√†ng ',\n",
        "        'beautiful': u' ƒë·∫πp tuy·ªát v·ªùi ', u' tl ': u' tr·∫£ l·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ',u' order ': u' ƒë·∫∑t h√†ng ',\n",
        "        'ch·∫•t lg': u' ch·∫•t l∆∞·ª£ng ',u' sd ': u' s·ª≠ d·ª•ng ',u' dt ': u' ƒëi·ªán tho·∫°i ',u' nt ': u' nh·∫Øn tin ',u' tl ': u' tr·∫£ l·ªùi ',u' s√†i ': u' x√†i ',u'bjo':u' bao gi·ªù ',\n",
        "        'thik': u' th√≠ch ',u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',u'qu·∫£ ng ':u' qu·∫£ng  ',\n",
        "        'dep': u' ƒë·∫πp ',u' xau ': u' x·∫•u ','delicious': u' ngon ', u'h√†g': u' h√†ng ', u'q·ªßa': u' qu·∫£ ',\n",
        "        'iu': u' y√™u ','fake': u' gi·∫£ m·∫°o ', 'trl': 'tr·∫£ l·ªùi', '><': u' positive ',\n",
        "        ' por ': u' t·ªá ',' poor ': u' t·ªá ', 'ib':u' nh·∫Øn tin ', 'rep':u' tr·∫£ l·ªùi ',u'fback':' feedback ','fedback':' feedback ',\n",
        "        #d∆∞·ªõi 3* quy v·ªÅ 1*, tr√™n 3* quy v·ªÅ 5*\n",
        "        '6 sao': ' 5star ','6 star': ' 5star ', '5star': ' 5star ','5 sao': ' 5star ','5sao': ' 5star ',\n",
        "        'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 1star ','2sao':' 1star ',\n",
        "        '2 starstar':' 1star ','1star': ' 1star ', '0 sao': ' 1star ', '0star': ' 1star ',}\n",
        "\n",
        "    for k, v in replace_list.items():\n",
        "        text = text.replace(k, v)\n",
        "\n",
        "    # chuyen punctuation th√†nh space\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "    texts = text.split()\n",
        "    len_text = len(texts)\n",
        "\n",
        "    texts = [t.replace('_', ' ') for t in texts]\n",
        "    for i in range(len_text):\n",
        "        cp_text = texts[i]\n",
        "        if cp_text in not_list: # X·ª≠ l√Ω v·∫•n ƒë·ªÅ ph·ªß ƒë·ªãnh (VD: √°o n√†y ch·∫≥ng ƒë·∫πp--> √°o n√†y notpos)\n",
        "            numb_word = 2 if len_text - i - 1 >= 4 else len_text - i - 1\n",
        "\n",
        "            for j in range(numb_word):\n",
        "                if texts[i + j + 1] in pos_list:\n",
        "                    texts[i] = 'notpos'\n",
        "                    texts[i + j + 1] = ''\n",
        "\n",
        "                if texts[i + j + 1] in nag_list:\n",
        "                    texts[i] = 'notnag'\n",
        "                    texts[i + j + 1] = ''\n",
        "        else: #Th√™m feature cho nh·ªØng sentiment words (√°o n√†y ƒë·∫πp--> √°o n√†y ƒë·∫πp positive)\n",
        "            if cp_text in pos_list:\n",
        "                texts.append('positive')\n",
        "            elif cp_text in nag_list:\n",
        "                texts.append('nagative')\n",
        "\n",
        "    text = u' '.join(texts)\n",
        "\n",
        "    #remove n·ªët nh·ªØng k√Ω t·ª± th·ª´a th√£i\n",
        "    text = text.replace(u'\"', u' ')\n",
        "    text = text.replace(u'Ô∏è', u'')\n",
        "    text = text.replace('üèª','')\n",
        "    return text"
      ],
      "metadata": {
        "id": "g9_NL-qIYu4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __one_hot__(labels):\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == \"POS\":\n",
        "      labels[i] = 2\n",
        "    elif labels[i] == \"NEU\":\n",
        "      labels[i] = 1\n",
        "    else:\n",
        "      labels[i] = 0\n",
        "  return labels"
      ],
      "metadata": {
        "id": "b-Lnh-26Rj5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_path = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/train.txt\"\n",
        "test_path = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/test.txt\"\n",
        "# path = (training_path, test_path)\n",
        "# path = training_path\n",
        "# print(path)\n",
        "reviews_train, labels_train = __getData__(training_path)\n",
        "reviews_test, labels_test = __getData__(test_path)\n",
        "labels_train = __one_hot__(labels_train)\n",
        "labels_test = __one_hot__(labels_test)\n"
      ],
      "metadata": {
        "id": "wYb6e28keClH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write to a new file\n",
        "def __write__(path, contents):\n",
        "  if os.path.exists(path):\n",
        "    os.remove(path)\n",
        "  f = open(path, \"a\")\n",
        "  for content in contents:\n",
        "    f.write(normalize_text(str(content)))\n",
        "    f.write(\"\\n\")\n",
        "  f.close()\n",
        "  print(f\"Write to file: {path.split('/')[-1]} done!\")"
      ],
      "metadata": {
        "id": "XHc8EbfZPjVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_filtered_test_X = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/filtered_test_X.txt\"\n",
        "path_filtered_test_Y = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/filtered_test_Y.txt\"\n",
        "path_filtered_training_X = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/filtered_training_X.txt\"\n",
        "path_filtered_training_Y = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/filtered_training_Y.txt\"\n",
        "__write__(path_filtered_training_X, reviews_train)\n",
        "__write__(path_filtered_training_Y, labels_train)\n",
        "__write__(path_filtered_test_X, reviews_test)\n",
        "__write__(path_filtered_test_Y, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKe8Su3mgqCY",
        "outputId": "2069f6ac-ea10-4123-a10e-655ea2ea271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write to file: filtered_training_X.txt done!\n",
            "Write to file: filtered_training_Y.txt done!\n",
            "Write to file: filtered_test_X.txt done!\n",
            "Write to file: filtered_test_Y.txt done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and predict using Bayes and N_grams"
      ],
      "metadata": {
        "id": "F_bEU3Z1UHQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __get_filtered_data__(path, store):\n",
        "    f = open(path, \"r\")\n",
        "    sentences = f.read()\n",
        "    lst_data = sentences.split(\"\\n\")\n",
        "    lst_data.pop()\n",
        "    for data in lst_data:\n",
        "        store.append(data)\n",
        "    f.close()      \n",
        "    return store"
      ],
      "metadata": {
        "id": "i0s8LvO9bnNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_train_X = []\n",
        "filtered_train_Y = []\n",
        "filtered_test_X = []\n",
        "filtered_test_Y = []\n",
        "\n",
        "filtered_train_X = __get_filtered_data__(path_filtered_training_X, filtered_train_X)\n",
        "filtered_train_Y = __get_filtered_data__(path_filtered_training_Y, filtered_train_Y)\n",
        "filtered_test_X = __get_filtered_data__(path_filtered_test_X, filtered_test_X)\n",
        "filtered_test_Y = __get_filtered_data__(path_filtered_test_Y, filtered_test_Y)\n",
        "filtered_train_X, filtered_test_X, filtered_train_Y, filtered_test_Y = train_test_split(\n",
        "    filtered_train_X, filtered_train_Y, test_size = 0.05, random_state = 23)"
      ],
      "metadata": {
        "id": "_3bzzgkNU_Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for N in range(1,5):\n",
        "    \n",
        "    # convert training data to bag of words\n",
        "    cv = CountVectorizer(analyzer = 'word',ngram_range=(1,N))\n",
        "    X_train_cv = cv.fit_transform(reviews_train)\n",
        "    X_test_cv = cv.transform(reviews_test)\n",
        "    \n",
        "    # train model and generate predictions\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X_train_cv, labels_train)\n",
        "    y_pred = clf.predict(X_test_cv)\n",
        "    \n",
        "    # compute f-1 score\n",
        "    score = np.round(f1_score(labels_test, y_pred, average='micro'),4)\n",
        "    print('F-1 score of model with n-gram range of {}: {}'.format((1,N), score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNflYgiFcS5h",
        "outputId": "1328ecad-fc80-484f-9b1d-8163262faa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-1 score of model with n-gram range of (1, 1): 0.6733\n",
            "F-1 score of model with n-gram range of (1, 2): 0.6867\n",
            "F-1 score of model with n-gram range of (1, 3): 0.6752\n",
            "F-1 score of model with n-gram range of (1, 4): 0.6648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict(cv.transform([\"Qu√°n b√¨nh th∆∞·ªùng, ƒë·ªì ƒÉn kh√¥ng ngon l·∫Øm\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XArB7rkd5aa",
        "outputId": "622391bf-a331-44c1-e638-4f1a40bb4d46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 300\n",
        "embedding_size = 128\n",
        "num_classes = 3\n",
        "filter_sizes = 3\n",
        "num_filters = 150\n",
        "epochs = 50\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.5"
      ],
      "metadata": {
        "id": "QUy9_EFX-WR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import gensim.models.keyedvectors as word2vec\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers \n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "A3QkrDI46ukH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_path = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/train.txt\"\n",
        "test_path = \"/content/drive/MyDrive/AI projects/Sentiment Classification/VLSP2016_SA/test.txt\"\n",
        "path = (training_path, test_path)\n",
        "\n",
        "reviews, labels = __getData__(path)\n",
        "\n",
        "input_gensim = []\n",
        "for review in reviews:\n",
        "    input_gensim.append(review.split())\n",
        "    \n",
        "model = Word2Vec(input_gensim, size=128, window=5, min_count=0, workers=4, sg=1)\n",
        "model.wv.save(\"word.model\")"
      ],
      "metadata": {
        "id": "E8rWft2x6tAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_embedding = word2vec.KeyedVectors.load('./word.model')\n",
        "\n",
        "word_labels = []\n",
        "max_seq = 300\n",
        "embedding_size = 128\n",
        "\n",
        "for word in model_embedding.vocab.keys():\n",
        "    word_labels.append(word)\n",
        "    \n",
        "def comment_embedding(comment):\n",
        "    matrix = np.zeros((max_seq, embedding_size))\n",
        "    words = comment.split()\n",
        "    lencmt = len(words)\n",
        "\n",
        "    for i in range(max_seq):\n",
        "        indexword = i % lencmt\n",
        "        if (max_seq - i < lencmt):\n",
        "            break\n",
        "        if(words[indexword] in word_labels):\n",
        "            matrix[i] = model_embedding[words[indexword]]\n",
        "    matrix = np.array(matrix)\n",
        "    return matrix\n"
      ],
      "metadata": {
        "id": "6GaVds1Z7VbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DICT_LABELS = {\n",
        "    \"NEU\": [1,0,0],\n",
        "    \"POS\": [0,1,0],\n",
        "    \"NEG\": [0,0,1]\n",
        "}"
      ],
      "metadata": {
        "id": "rhB4uTFh9Ain"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "label_data = []\n",
        "\n",
        "for x in tqdm(reviews):\n",
        "    train_data.append(comment_embedding(x))\n",
        "train_data = np.array(train_data)\n",
        "\n",
        "for y in tqdm(labels):\n",
        "  # label_ = np.zeros(3)\n",
        "  # try:\n",
        "  #     label_[int(y)] = 1\n",
        "  # except:\n",
        "  #     label_[0] = 1\n",
        "  label_data.append(DICT_LABELS[y])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDvDiOPN8SV2",
        "outputId": "bea86d13-fd10-4f78-ed5d-33f3c8df06eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6150/6150 [01:23<00:00, 73.24it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6150/6150 [00:00<00:00, 1916986.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = train_data.reshape(train_data.shape[0], sequence_length, embedding_size, 1).astype('float32')\n",
        "y_train = np.array(label_data)\n",
        "\n",
        "# Define model\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Convolution2D(num_filters, (filter_sizes, embedding_size),\n",
        "                        padding='valid',\n",
        "                        input_shape=(sequence_length, embedding_size, 1), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(198, 1)))\n",
        "model.add(layers.Dropout(dropout_rate))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(126, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "# Train model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "kC7bRix9-ME-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train[:5100], y_train[:5100], batch_size = batch_size, verbose=1, epochs=epochs, validation_data=(x_train[5100:], y_train[5100:]))\n",
        "\n",
        "model.save('models.h5')\n"
      ],
      "metadata": {
        "id": "RXm2M7Ih-h_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9sk6QHmGd-9l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}